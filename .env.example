# MadSpark LLM Provider Configuration
# Copy this file to .env and update with your values

# Google Cloud API (required for Gemini and PDF/URL processing)
# SECURITY: Replace with real API key
GOOGLE_API_KEY=your-api-key-here-replace-with-real-key
GOOGLE_GENAI_MODEL=gemini-2.5-flash

# LLM Provider Configuration
# Provider selection: auto (default), ollama, gemini
MADSPARK_LLM_PROVIDER=auto
# Model tier: fast (4B), balanced (12B), quality (gemini)
MADSPARK_MODEL_TIER=fast
# Enable automatic fallback from Ollama to Gemini
MADSPARK_FALLBACK_ENABLED=true

# Ollama Settings (for local inference - FREE)
# Requires: ollama serve running and models pulled
# ollama pull gemma3:4b-it-qat
# ollama pull gemma3:12b-it-qat
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL_FAST=gemma3:4b-it-qat
OLLAMA_MODEL_BALANCED=gemma3:12b-it-qat

# LLM Response Cache Settings
MADSPARK_CACHE_ENABLED=true
MADSPARK_CACHE_TTL=86400
# Cache uses ~/.cache/madspark/llm by default (absolute path)
# MADSPARK_CACHE_DIR=~/.cache/madspark/llm

# System Configuration (legacy - will be removed)
# NOTE: Set to 'mock' for testing without API key, 'api' for real API calls
# Default behavior: auto-detects based on API key presence
# MADSPARK_MODE=api
MAX_CONCURRENT_AGENTS=10
