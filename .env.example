# MadSpark LLM Provider Configuration
# Copy this file to .env and update with your values

# Google Cloud API (required for Gemini and PDF/URL processing)
# SECURITY: Replace with real API key
GOOGLE_API_KEY=your-api-key-here-replace-with-real-key
GOOGLE_GENAI_MODEL=gemini-3-flash-preview

# LLM Provider Configuration
# Provider selection: auto (default), ollama, gemini
MADSPARK_LLM_PROVIDER=auto
# Model tier: fast (4B), balanced (12B, default), quality (gemini)
MADSPARK_MODEL_TIER=balanced
# Enable automatic fallback from Ollama to Gemini
MADSPARK_FALLBACK_ENABLED=true

# Ollama Settings (for local inference - FREE)
# Requires: ollama serve running and models pulled
# ollama pull gemma3:4b
# ollama pull gemma3:12b
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL_BALANCED=gemma3:12b
OLLAMA_MODEL_FAST=gemma3:4b

# LLM Response Cache Settings
MADSPARK_CACHE_ENABLED=true
MADSPARK_CACHE_TTL=86400
# Cache uses ~/.cache/madspark/llm by default (absolute path)
# MADSPARK_CACHE_DIR=~/.cache/madspark/llm

# System Configuration (legacy - will be removed)
# NOTE: Set to 'mock' for testing without API key, 'api' for real API calls
# Default behavior: auto-detects based on API key presence
# MADSPARK_MODE=api
MAX_CONCURRENT_AGENTS=10

# Security Configuration
# Comma-separated list of allowed origins for CORS.
# Defaults to http://localhost:3000,http://127.0.0.1:3000 if not set.
# MADSPARK_CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,https://my-production-domain.com
