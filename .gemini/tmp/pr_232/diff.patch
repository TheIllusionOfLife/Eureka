diff --git a/.env.example b/.env.example
index 3582d749..56f1f028 100644
--- a/.env.example
+++ b/.env.example
@@ -4,7 +4,7 @@
 # Google Cloud API (required for Gemini and PDF/URL processing)
 # SECURITY: Replace with real API key
 GOOGLE_API_KEY=your-api-key-here-replace-with-real-key
-GOOGLE_GENAI_MODEL=gemini-2.5-flash
+GOOGLE_GENAI_MODEL=gemini-3-flash-preview
 
 # LLM Provider Configuration
 # Provider selection: auto (default), ollama, gemini
diff --git a/CLAUDE.md b/CLAUDE.md
index 1dd5a6cf..77cec3d5 100644
--- a/CLAUDE.md
+++ b/CLAUDE.md
@@ -133,7 +133,7 @@ ms "topic" --provider gemini       # Force cloud API
 # Model tier (default: balanced)
 ms "topic" --model-tier fast       # gemma3:4b (quick, ~3.3GB)
 ms "topic" --model-tier balanced   # gemma3:12b (default, ~8.1GB)
-ms "topic" --model-tier quality    # gemini-2.5-flash (cloud, best)
+ms "topic" --model-tier quality    # gemini-3-flash-preview (cloud, best)
 
 # Cache control (enabled by default)
 ms "topic" --no-cache              # Disable caching
@@ -251,7 +251,7 @@ from google import genai
 genai_client = genai.Client()
 
 # Define your model and prompt
-model_name = "gemini-1.5-flash"  # or your preferred model
+model_name = "gemini-3-flash-preview"  # or your preferred model
 prompt = "Your prompt text here"
 
 # Configure the request
@@ -322,7 +322,7 @@ config = types.GenerateContentConfig(
 # 3. Make API call
 client = genai.Client()
 response = client.models.generate_content(
-    model="gemini-2.5-flash",
+    model="gemini-3-flash-preview",
     contents="...",
     config=config
 )
diff --git a/README.md b/README.md
index 8c357e29..4fd663df 100644
--- a/README.md
+++ b/README.md
@@ -71,7 +71,7 @@ For automated environments where interactive prompts aren't available:
 ```bash
 # Create .env file with your API key (root directory)
 echo 'GOOGLE_API_KEY="YOUR_API_KEY_HERE"' > .env
-echo 'GOOGLE_GENAI_MODEL="gemini-2.5-flash"' >> .env
+echo 'GOOGLE_GENAI_MODEL="gemini-3-flash-preview"' >> .env
 
 # Verify configuration
 mad_spark config --status
@@ -319,7 +319,7 @@ export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"
 
 # Configure API (use root .env file)
 echo 'GOOGLE_API_KEY="YOUR_API_KEY_HERE"' > .env
-echo 'GOOGLE_GENAI_MODEL="gemini-2.5-flash"' >> .env
+echo 'GOOGLE_GENAI_MODEL="gemini-3-flash-preview"' >> .env
 ```
 
 </details>
diff --git a/docs/ARCHITECTURE.md b/docs/ARCHITECTURE.md
index a1ad0374..dec17ebe 100644
--- a/docs/ARCHITECTURE.md
+++ b/docs/ARCHITECTURE.md
@@ -720,7 +720,7 @@ services:
 | Variable | Description | Default | Required |
 |----------|-------------|---------|----------|
 | `GOOGLE_API_KEY` | Gemini API key | - | Yes (except mock) |
-| `GOOGLE_GENAI_MODEL` | Model name | gemini-2.5-flash | No |
+| `GOOGLE_GENAI_MODEL` | Model name | gemini-3-flash-preview | No |
 | `MADSPARK_MODE` | Operational mode | production | No |
 | `LOG_LEVEL` | Logging verbosity | INFO | No |
 | `REDIS_URL` | Redis connection | - | No |
@@ -816,7 +816,7 @@ Eureka/
     "tags": ["array", "of", "tags"],
     "timestamp": "ISO-8601",
     "metadata": {
-      "model_used": "gemini-2.5-flash",
+      "model_used": "gemini-3-flash-preview",
       "temperature": 0.9,
       "token_count": 1500
     }
diff --git a/docs/WEB_INTERFACE_GUIDE.md b/docs/WEB_INTERFACE_GUIDE.md
index d5d6fb6e..7055654c 100644
--- a/docs/WEB_INTERFACE_GUIDE.md
+++ b/docs/WEB_INTERFACE_GUIDE.md
@@ -23,7 +23,7 @@ Services will start on:
 Ensure your `.env` file contains:
 ```env
 GOOGLE_API_KEY=your_api_key_here
-GOOGLE_GENAI_MODEL=gemini-2.5-flash
+GOOGLE_GENAI_MODEL=gemini-3-flash-preview
 ```
 
 ## User Interface Overview
diff --git a/scripts/setup.sh b/scripts/setup.sh
index 23884c8c..4ddf4c48 100755
--- a/scripts/setup.sh
+++ b/scripts/setup.sh
@@ -14,7 +14,7 @@ create_mock_env() {
     cat > "$ENV_FILE" << EOF
 # Google API Configuration
 GOOGLE_API_KEY="YOUR_API_KEY_HERE"
-GOOGLE_GENAI_MODEL="gemini-2.5-flash"
+GOOGLE_GENAI_MODEL="gemini-3-flash-preview"
 # Running in mock mode - no API key configured
 MADSPARK_MODE="mock"
 EOF
@@ -119,7 +119,7 @@ if [ "$API_KEY_CONFIGURED" = false ]; then
                 cat > "$ENV_FILE" << EOF
 # Google API Configuration
 GOOGLE_API_KEY="$api_key"
-GOOGLE_GENAI_MODEL="gemini-2.5-flash"
+GOOGLE_GENAI_MODEL="gemini-3-flash-preview"
 EOF
                 echo -e "${GREEN}âœ… API key configured successfully!${NC}"
                 API_KEY_CONFIGURED=true
diff --git a/src/madspark/bin/mad_spark_config b/src/madspark/bin/mad_spark_config
index 92597fb3..831c086b 100755
--- a/src/madspark/bin/mad_spark_config
+++ b/src/madspark/bin/mad_spark_config
@@ -15,6 +15,17 @@ BLUE = '\033[0;34m'
 PURPLE = '\033[0;35m'
 NC = '\033[0m'  # No Color
 
+# Try to import the model constant from the package
+try:
+    # Add src to path for import
+    _script_dir = Path(__file__).resolve().parent
+    _src_dir = _script_dir.parent.parent.parent
+    sys.path.insert(0, str(_src_dir))
+    from madspark.llm.models import GEMINI_MODEL_DEFAULT
+except ImportError:
+    # Fallback if import fails
+    GEMINI_MODEL_DEFAULT = "gemini-3-flash-preview"
+
 def get_project_root():
     """Find project root by looking for README.md or run.py"""
     current = Path(__file__).resolve()
@@ -41,7 +52,7 @@ def write_env_file(env_path, config):
     lines = []
     lines.append("# Google API Configuration")
     lines.append(f'GOOGLE_API_KEY="{config.get("GOOGLE_API_KEY", "YOUR_API_KEY_HERE")}"')
-    lines.append(f'GOOGLE_GENAI_MODEL="{config.get("GOOGLE_GENAI_MODEL", "gemini-2.5-flash")}"')
+    lines.append(f'GOOGLE_GENAI_MODEL="{config.get("GOOGLE_GENAI_MODEL", GEMINI_MODEL_DEFAULT)}"')
     
     # Add MADSPARK_MODE if in mock mode
     if config.get("MADSPARK_MODE") == "mock":
@@ -102,8 +113,8 @@ def main():
         print(f"{PURPLE}Current Configuration:{NC}")
         print(f"  API Key: {'âœ… Configured' if validate_api_key(current_key) else 'âŒ Not configured'}")
         print(f"  Mode: {'Mock' if current_mode == 'mock' or not validate_api_key(current_key) else 'API'}")
-        print(f"  Model: {config.get('GOOGLE_GENAI_MODEL', 'gemini-2.5-flash')}")
-        
+        print(f"  Model: {config.get('GOOGLE_GENAI_MODEL', GEMINI_MODEL_DEFAULT)}")
+
         if not sys.stdin.isatty() and not show_status_only:
             print()
             print(f"{YELLOW}ðŸ’¡ To configure interactively, run this command in a terminal{NC}")
@@ -181,8 +192,8 @@ def main():
         print(f"{PURPLE}Current Configuration:{NC}")
         print(f"  API Key: {'âœ… Configured' if validate_api_key(current_key) else 'âŒ Not configured'}")
         print(f"  Mode: {'Mock' if current_mode == 'mock' or not validate_api_key(current_key) else 'API'}")
-        print(f"  Model: {config.get('GOOGLE_GENAI_MODEL', 'gemini-2.5-flash')}")
-    
+        print(f"  Model: {config.get('GOOGLE_GENAI_MODEL', GEMINI_MODEL_DEFAULT)}")
+
     elif choice == "4" or choice == "":
         print("Exiting...")
         sys.exit(0)
diff --git a/src/madspark/core/coordinator_batch.py b/src/madspark/core/coordinator_batch.py
index 34002b86..2c469465 100644
--- a/src/madspark/core/coordinator_batch.py
+++ b/src/madspark/core/coordinator_batch.py
@@ -28,7 +28,8 @@
     MEANINGFUL_IMPROVEMENT_SCORE_DELTA,
     LOGICAL_INFERENCE_CONFIDENCE_THRESHOLD,
     TOKENS_PER_WORD_ESTIMATE,
-    OUTPUT_TOKENS_PER_INFERENCE
+    OUTPUT_TOKENS_PER_INFERENCE,
+    DEFAULT_GOOGLE_GENAI_MODEL,
 )
 from madspark.utils.temperature_control import TemperatureManager
 from madspark.utils.novelty_filter import NoveltyFilter
@@ -364,7 +365,7 @@ def _run_workflow_internal(
                         context=batch_context,
                         success=True,
                         tokens_used=input_tokens + output_tokens,
-                        model_name="gemini-2.5-flash"  # LogicalInferenceEngine uses flash model
+                        model_name=DEFAULT_GOOGLE_GENAI_MODEL
                     )
                 except Exception as e:
                     monitor.end_batch_call(
diff --git a/src/madspark/llm/models.py b/src/madspark/llm/models.py
index 7bad414f..c1c32678 100644
--- a/src/madspark/llm/models.py
+++ b/src/madspark/llm/models.py
@@ -22,8 +22,11 @@
 # Gemini Models (Cloud Inference)
 # =============================================================================
 
-# Default Gemini model
-GEMINI_MODEL_DEFAULT: str = "gemini-2.5-flash"
+# Default Gemini model (Gemini 3 Flash - preview)
+# Pricing: $0.50 / $3.00 per million tokens (input/output)
+# Context: 1M input / 64k output
+# Knowledge cutoff: January 2025
+GEMINI_MODEL_DEFAULT: str = "gemini-3-flash-preview"
 
 # =============================================================================
 # Default Configuration
diff --git a/src/madspark/llm/providers/gemini.py b/src/madspark/llm/providers/gemini.py
index 1e731c21..eef6364c 100644
--- a/src/madspark/llm/providers/gemini.py
+++ b/src/madspark/llm/providers/gemini.py
@@ -459,11 +459,21 @@ def extract_content_from_files(
 
     def get_cost_per_token(self) -> float:
         """
-        Gemini 2.5 Flash approximate pricing.
+        Get cost per token based on model.
 
         Returns:
-            Cost per token in USD (approximate average)
+            Cost per token in USD (approximate average of input/output)
         """
-        # ~$0.075 per million input tokens + $0.30 per million output tokens
-        # Simplified average
-        return 0.0000002  # $0.20 per million tokens average
+        # Model-specific pricing (average of input + output costs)
+        model_costs = {
+            # Gemini 3 Flash: $0.50 input + $3.00 output = $1.75 average per 1M
+            "gemini-3-flash-preview": 0.00000175,
+            # Gemini 3 Pro: $2.00 input + $12.00 output = $7.00 average per 1M
+            "gemini-3-pro-preview": 0.000007,
+            # Gemini 2.5 Flash: $0.075 input + $0.30 output = $0.1875 average per 1M
+            "gemini-2.5-flash": 0.0000001875,
+            # Legacy defaults
+            "gemini-1.5-flash": 0.0000001875,
+            "gemini-1.5-pro": 0.000003125,
+        }
+        return model_costs.get(self._model, 0.00000175)  # Default to Gemini 3 Flash
diff --git a/src/madspark/utils/constants.py b/src/madspark/utils/constants.py
index 653650bf..94fbbd26 100644
--- a/src/madspark/utils/constants.py
+++ b/src/madspark/utils/constants.py
@@ -86,7 +86,7 @@
     DEFAULT_GOOGLE_GENAI_MODEL = GEMINI_MODEL_DEFAULT
 except ImportError:
     # Fallback for environments where llm package isn't available
-    DEFAULT_GOOGLE_GENAI_MODEL = "gemini-2.5-flash"
+    DEFAULT_GOOGLE_GENAI_MODEL = "gemini-3-flash-preview"
 
 # Temperature defaults for specific agents/functions
 DEFAULT_CRITIC_TEMPERATURE = 0.3
diff --git a/src/madspark/utils/pricing_config.py b/src/madspark/utils/pricing_config.py
index 6e98d474..b10c50ba 100644
--- a/src/madspark/utils/pricing_config.py
+++ b/src/madspark/utils/pricing_config.py
@@ -9,6 +9,21 @@
 # Token costs configuration (per 1K tokens)
 # These values should be updated based on actual model pricing
 TOKEN_COSTS: Dict[str, Dict[str, float]] = {
+    # Gemini 3 Flash (current default)
+    "gemini-3-flash-preview": {
+        "input": 0.0005,    # $0.50 per 1M input tokens
+        "output": 0.003     # $3.00 per 1M output tokens
+    },
+    # Gemini 3 Pro (higher quality, more expensive)
+    "gemini-3-pro-preview": {
+        "input": 0.002,     # $2.00 per 1M input tokens (<200k)
+        "output": 0.012     # $12.00 per 1M output tokens (<200k)
+    },
+    # Legacy Gemini models
+    "gemini-2.5-flash": {
+        "input": 0.000075,  # $0.075 per 1M input tokens
+        "output": 0.0003    # $0.30 per 1M output tokens
+    },
     "gemini-1.5-pro": {
         "input": 0.00125,   # $1.25 per 1M input tokens
         "output": 0.005     # $5.00 per 1M output tokens
@@ -17,7 +32,7 @@
         "input": 0.000075,  # $0.075 per 1M input tokens
         "output": 0.0003    # $0.30 per 1M output tokens
     },
-    # Add more models as needed
+    # OpenAI models (for reference)
     "gpt-4": {
         "input": 0.03,      # $30 per 1M input tokens
         "output": 0.06      # $60 per 1M output tokens
@@ -29,7 +44,7 @@
 }
 
 # Default model for cost estimation when actual model is unknown
-DEFAULT_PRICING_MODEL = "gemini-1.5-pro"
+DEFAULT_PRICING_MODEL = "gemini-3-flash-preview"
 
 # Estimated output token ratio (output tokens as percentage of input tokens)
 DEFAULT_OUTPUT_RATIO = 0.3  # 30% output tokens
diff --git a/tests/conftest.py b/tests/conftest.py
index 06b27c8f..3accbaa8 100644
--- a/tests/conftest.py
+++ b/tests/conftest.py
@@ -6,6 +6,22 @@
 import os
 import pytest
 
+# ============================================================================
+# Test Model Constants
+# ============================================================================
+# Use these constants in tests instead of hardcoding model names.
+# This makes tests resilient to model upgrades.
+# For direct imports in test files, use: from test_constants import TEST_MODEL_NAME
+
+# Generic test model name - use for mocks where the exact name doesn't matter
+TEST_MODEL_NAME = "gemini-test-model"
+
+# Import the actual default model for tests that need it
+try:
+    from madspark.llm.models import GEMINI_MODEL_DEFAULT
+except ImportError:
+    GEMINI_MODEL_DEFAULT = "gemini-3-flash-preview"
+
 
 def pytest_configure(config):
     """
diff --git a/tests/test_advocate_batch.py b/tests/test_advocate_batch.py
index c33ef46b..ea47c68e 100644
--- a/tests/test_advocate_batch.py
+++ b/tests/test_advocate_batch.py
@@ -1,6 +1,7 @@
 """Tests for batch advocate functionality."""
 import pytest
 from unittest.mock import Mock, patch
+from .test_constants import TEST_MODEL_NAME
 
 try:
     from madspark.agents.advocate import advocate_ideas_batch
@@ -18,7 +19,7 @@ class TestAdvocateBatch:
     @patch('madspark.agents.advocate.get_model_name')
     def test_advocate_ideas_batch_single(self, mock_model_name, mock_client):
         """Test batch advocate with single idea."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         # Mock response
         mock_response = Mock()
@@ -53,7 +54,7 @@ def test_advocate_ideas_batch_single(self, mock_model_name, mock_client):
     @patch('madspark.agents.advocate.get_model_name')
     def test_advocate_ideas_batch_multiple(self, mock_model_name, mock_client):
         """Test batch advocate with multiple ideas."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         # Mock response for 3 ideas
         mock_response = Mock()
@@ -115,7 +116,7 @@ def test_advocate_ideas_batch_empty_list(self, mock_client):
     @patch('madspark.agents.advocate.get_model_name')
     def test_advocate_ideas_batch_invalid_json(self, mock_model_name, mock_client):
         """Test handling of invalid JSON response - now recovers with placeholders."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         mock_response = Mock()
         mock_response.text = "Invalid JSON response"
@@ -139,7 +140,7 @@ def test_advocate_ideas_batch_invalid_json(self, mock_model_name, mock_client):
     @patch('madspark.agents.advocate.get_model_name')
     def test_advocate_ideas_batch_api_error(self, mock_model_name, mock_client):
         """Test handling of API errors."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         mock_client.models.generate_content.side_effect = Exception("API Error")
         
         ideas = [{"idea": "Test", "evaluation": "Test eval"}]
@@ -153,7 +154,7 @@ def test_advocate_ideas_batch_api_error(self, mock_model_name, mock_client):
     @patch('madspark.agents.advocate.get_model_name')
     def test_advocate_ideas_batch_formatted_output(self, mock_model_name, mock_client):
         """Test that batch results include formatted text output."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         mock_response = Mock()
         mock_response.text = '''[{
diff --git a/tests/test_config_command.py b/tests/test_config_command.py
index 7bd29e01..194b9de5 100644
--- a/tests/test_config_command.py
+++ b/tests/test_config_command.py
@@ -50,16 +50,16 @@ def test_config_script_functions():
     test_env_content = '''
 # Google API Configuration
 GOOGLE_API_KEY="AIzaSyD-test-key"
-GOOGLE_GENAI_MODEL="gemini-2.5-flash"
+GOOGLE_GENAI_MODEL="gemini-3-flash-preview"
 MADSPARK_MODE="mock"
 '''
-    
+
     read_env_file = namespace['read_env_file']
     with patch('pathlib.Path.exists', return_value=True):
         with patch('builtins.open', mock_open(read_data=test_env_content)):
             config = read_env_file(Path("test.env"))
             assert config["GOOGLE_API_KEY"] == "AIzaSyD-test-key"
-            assert config["GOOGLE_GENAI_MODEL"] == "gemini-2.5-flash"
+            assert config["GOOGLE_GENAI_MODEL"] == "gemini-3-flash-preview"
             assert config["MADSPARK_MODE"] == "mock"
 
 def test_setup_default_behavior():
diff --git a/tests/test_constants.py b/tests/test_constants.py
new file mode 100644
index 00000000..ec9eface
--- /dev/null
+++ b/tests/test_constants.py
@@ -0,0 +1,15 @@
+"""
+Test constants for consistent model naming across tests.
+
+This module provides constants that can be imported by test files.
+Use these instead of hardcoding model names to make tests resilient to model upgrades.
+"""
+
+# Generic test model name - use for mocks where the exact name doesn't matter
+TEST_MODEL_NAME = "gemini-test-model"
+
+# Import the actual default model for tests that need it
+try:
+    from madspark.llm.models import GEMINI_MODEL_DEFAULT
+except ImportError:
+    GEMINI_MODEL_DEFAULT = "gemini-3-flash-preview"
diff --git a/tests/test_critic_router.py b/tests/test_critic_router.py
index d87cd2b7..e188bdfe 100644
--- a/tests/test_critic_router.py
+++ b/tests/test_critic_router.py
@@ -8,6 +8,7 @@
 import json
 import pytest
 from unittest.mock import patch, Mock
+from .test_constants import TEST_MODEL_NAME
 
 from madspark.schemas.evaluation import CriticEvaluations
 from madspark.llm.response import LLMResponse
@@ -131,7 +132,7 @@ def test_evaluate_ideas_logs_router_usage(self, caplog):
         mock_response = LLMResponse(
             text='[]',
             provider="gemini",
-            model="gemini-2.5-flash",
+            model="TEST_MODEL_NAME",
             tokens_used=200,
             latency_ms=500,
             cost=0.00004
diff --git a/tests/test_enhanced_reasoning_structured_output.py b/tests/test_enhanced_reasoning_structured_output.py
index 52efc6b6..6bb1f0d2 100644
--- a/tests/test_enhanced_reasoning_structured_output.py
+++ b/tests/test_enhanced_reasoning_structured_output.py
@@ -6,6 +6,7 @@
 
 import pytest
 from unittest.mock import Mock, patch
+from .test_constants import TEST_MODEL_NAME
 
 
 class TestDimensionScoreStructuredOutput:
@@ -17,7 +18,7 @@ def test_evaluate_dimension_uses_structured_output_config(self, mock_get_model):
         from madspark.core.enhanced_reasoning import MultiDimensionalEvaluator
         from google.genai import types
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         # Create mock GenAI client
         mock_client = Mock()
@@ -53,7 +54,7 @@ def test_evaluate_dimension_parses_json_response(self, mock_get_model):
         """Should parse JSON response instead of raw text."""
         from madspark.core.enhanced_reasoning import MultiDimensionalEvaluator
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         # Mock client that returns structured JSON
         mock_client = Mock()
@@ -78,7 +79,7 @@ def test_evaluate_dimension_handles_optional_reasoning(self, mock_get_model):
         """Should handle responses with optional reasoning field."""
         from madspark.core.enhanced_reasoning import MultiDimensionalEvaluator
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -102,7 +103,7 @@ def test_evaluate_dimension_clamps_score_to_range(self, mock_get_model):
         """Should clamp scores to dimension range."""
         from madspark.core.enhanced_reasoning import MultiDimensionalEvaluator
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -127,7 +128,7 @@ def test_evaluate_dimension_raises_on_invalid_json(self, mock_get_model):
         """Should raise clear error on invalid JSON response."""
         from madspark.core.enhanced_reasoning import MultiDimensionalEvaluator
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -149,7 +150,7 @@ def test_evaluate_dimension_raises_on_missing_score(self, mock_get_model):
         """Should raise clear error when score field is missing."""
         from madspark.core.enhanced_reasoning import MultiDimensionalEvaluator
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -211,7 +212,7 @@ def test_multi_dimensional_evaluation_uses_structured_output(self, mock_get_mode
         """Full evaluation should use structured output for all dimensions."""
         from madspark.core.enhanced_reasoning import MultiDimensionalEvaluator
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         # Return different scores for different dimensions (default has 7 dimensions)
@@ -253,7 +254,7 @@ def test_evaluator_still_works_with_existing_tests(self, mock_get_model):
         """Existing test patterns should still work."""
         from madspark.core.enhanced_reasoning import MultiDimensionalEvaluator
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
diff --git a/tests/test_evaluation_data_flow.py b/tests/test_evaluation_data_flow.py
index a207145e..e294ed17 100644
--- a/tests/test_evaluation_data_flow.py
+++ b/tests/test_evaluation_data_flow.py
@@ -8,6 +8,7 @@
 to the improvement step for better-informed improvements.
 """
 from unittest.mock import Mock, patch
+from .test_constants import TEST_MODEL_NAME
 
 
 class TestFormatLogicalInferenceForPrompt:
@@ -394,7 +395,7 @@ def test_batch_prompt_includes_initial_score(self, mock_client, mock_model_name,
         """Verify batch prompt includes initial_score when provided."""
         from madspark.agents.idea_generator import improve_ideas_batch
 
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         mock_types.GenerateContentConfig.return_value = Mock()
         mock_response = Mock()
         mock_response.text = '[{"idea_index": 0, "improved_idea": "Better idea", "key_improvements": []}]'
@@ -423,7 +424,7 @@ def test_batch_prompt_includes_dimension_scores(self, mock_client, mock_model_na
         """Verify batch prompt includes dimension_scores when provided."""
         from madspark.agents.idea_generator import improve_ideas_batch
 
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         mock_types.GenerateContentConfig.return_value = Mock()
         mock_response = Mock()
         mock_response.text = '[{"idea_index": 0, "improved_idea": "Better idea", "key_improvements": []}]'
@@ -456,7 +457,7 @@ def test_batch_prompt_includes_logical_inference(self, mock_client, mock_model_n
         """Verify batch prompt includes logical_inference when provided."""
         from madspark.agents.idea_generator import improve_ideas_batch
 
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         mock_types.GenerateContentConfig.return_value = Mock()
         mock_response = Mock()
         mock_response.text = '[{"idea_index": 0, "improved_idea": "Better idea", "key_improvements": []}]'
diff --git a/tests/test_execution_constants.py b/tests/test_execution_constants.py
index 5f92c650..d1c5715e 100644
--- a/tests/test_execution_constants.py
+++ b/tests/test_execution_constants.py
@@ -271,7 +271,9 @@ def test_no_hardcoded_model_names(self):
         source = inspect.getsource(module)
 
         # Count occurrences of hardcoded model name (should be minimal after migration)
-        hardcoded_count = source.count('"gemini-2.5-flash"') + source.count("'gemini-2.5-flash'")
+        # Note: We check for the current default model name. Legacy model names in
+        # pricing_config.py are intentionally kept for backward compatibility.
+        hardcoded_count = source.count('"gemini-3-flash-preview"') + source.count("'gemini-3-flash-preview'")
 
         # Should have at most 1 (in the constant definition itself)
         assert hardcoded_count <= 1, \
diff --git a/tests/test_idea_generator_router.py b/tests/test_idea_generator_router.py
index 28321b6c..c04207c4 100644
--- a/tests/test_idea_generator_router.py
+++ b/tests/test_idea_generator_router.py
@@ -6,6 +6,7 @@
 
 import json
 from unittest.mock import patch, Mock
+from .test_constants import TEST_MODEL_NAME
 
 from madspark.schemas.generation import GeneratedIdeas
 from madspark.llm.response import LLMResponse
@@ -157,7 +158,7 @@ def test_generate_ideas_logs_router_usage(self, caplog):
         mock_response = LLMResponse(
             text='[]',
             provider="gemini",
-            model="gemini-2.5-flash",
+            model="TEST_MODEL_NAME",
             tokens_used=300,
             latency_ms=700,
             cost=0.00006
diff --git a/tests/test_idea_improvement_batch.py b/tests/test_idea_improvement_batch.py
index 5a2821d1..10837be5 100644
--- a/tests/test_idea_improvement_batch.py
+++ b/tests/test_idea_improvement_batch.py
@@ -1,6 +1,7 @@
 """Tests for batch idea improvement functionality."""
 import pytest
 from unittest.mock import Mock, patch
+from .test_constants import TEST_MODEL_NAME
 
 try:
     from madspark.agents.idea_generator import improve_ideas_batch
@@ -18,7 +19,7 @@ class TestIdeaImprovementBatch:
     @patch('madspark.agents.idea_generator.get_model_name')
     def test_improve_ideas_batch_single(self, mock_model_name, mock_client):
         """Test batch improvement with single idea."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         # Mock response
         mock_response = Mock()
@@ -53,7 +54,7 @@ def test_improve_ideas_batch_single(self, mock_model_name, mock_client):
     @patch('madspark.agents.idea_generator.get_model_name')
     def test_improve_ideas_batch_multiple(self, mock_model_name, mock_client):
         """Test batch improvement with multiple ideas."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         # Mock response for 3 ideas
         mock_response = Mock()
@@ -126,7 +127,7 @@ def test_improve_ideas_batch_empty_list(self, mock_client):
     @patch('madspark.agents.idea_generator.get_model_name')
     def test_improve_ideas_batch_invalid_json(self, mock_model_name, mock_client):
         """Test handling of invalid JSON response."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         mock_response = Mock()
         mock_response.text = "Invalid JSON response"
@@ -147,7 +148,7 @@ def test_improve_ideas_batch_invalid_json(self, mock_model_name, mock_client):
     @patch('madspark.agents.idea_generator.get_model_name')
     def test_improve_ideas_batch_api_error(self, mock_model_name, mock_client):
         """Test handling of API errors."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         mock_client.models.generate_content.side_effect = Exception("API Error")
         
         ideas = [{
@@ -227,7 +228,7 @@ def test_improve_ideas_batch_validates_input(self):
     @patch('madspark.agents.idea_generator.get_model_name')
     def test_improve_ideas_batch_preserves_order(self, mock_model_name, mock_client):
         """Test that batch improvement preserves idea order."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         # Return in different order to test ordering
         mock_response = Mock()
diff --git a/tests/test_language_consistency.py b/tests/test_language_consistency.py
index eed6fbe8..28ec3816 100644
--- a/tests/test_language_consistency.py
+++ b/tests/test_language_consistency.py
@@ -8,6 +8,7 @@
 import sys
 import pytest
 from unittest.mock import Mock, patch
+from .test_constants import TEST_MODEL_NAME
 
 # Add src to path for imports
 sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))
@@ -122,7 +123,7 @@ def test_dimension_prompts_format_correctly_with_unicode(self):
     def test_actual_api_call_includes_language_instruction(self, mock_get_model):
         """Test that actual API calls include language instruction in the prompt."""
         # Arrange
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
         mock_client = Mock()
         mock_response = Mock()
         mock_response.text = '{"score": 7, "reasoning": "Good"}'
diff --git a/tests/test_llm_providers.py b/tests/test_llm_providers.py
index 9d799fe8..56747c9c 100644
--- a/tests/test_llm_providers.py
+++ b/tests/test_llm_providers.py
@@ -17,6 +17,7 @@
 from unittest.mock import Mock, patch  # noqa: E402
 from pydantic import BaseModel, Field  # noqa: E402
 from datetime import datetime  # noqa: E402
+from .test_constants import TEST_MODEL_NAME  # noqa: E402
 
 from madspark.llm.providers.ollama import OllamaProvider, OLLAMA_AVAILABLE  # noqa: E402
 from madspark.llm.providers.gemini import GeminiProvider, GENAI_AVAILABLE  # noqa: E402
@@ -191,7 +192,7 @@ def test_to_dict(self):
         response = LLMResponse(
             text="test",
             provider="gemini",
-            model="gemini-2.5-flash",
+            model="TEST_MODEL_NAME",
             tokens_used=100,
             latency_ms=500,
             cost=0.00002,
diff --git a/tests/test_llm_router.py b/tests/test_llm_router.py
index ab9e6835..45c75257 100644
--- a/tests/test_llm_router.py
+++ b/tests/test_llm_router.py
@@ -57,9 +57,10 @@ def mock_ollama_provider():
 @pytest.fixture
 def mock_gemini_provider():
     """Create mock Gemini provider."""
+    from .test_constants import TEST_MODEL_NAME
     provider = Mock()
     provider.provider_name = "gemini"
-    provider.model_name = "gemini-2.5-flash"
+    provider.model_name = TEST_MODEL_NAME
     provider.health_check.return_value = True
     provider.supports_multimodal = True
     provider.generate_structured.return_value = (
@@ -67,7 +68,7 @@ def mock_gemini_provider():
         LLMResponse(
             text='{"score": 7.0, "comment": "Good from Gemini"}',
             provider="gemini",
-            model="gemini-2.5-flash",
+            model=TEST_MODEL_NAME,
             tokens_used=100,
             latency_ms=500,
             cost=0.00002,
@@ -317,11 +318,12 @@ def test_health_status(
 
         status = router.health_status()
 
+        from .test_constants import TEST_MODEL_NAME
         assert status["ollama"]["available"] is True
         assert status["ollama"]["healthy"] is True
         assert status["ollama"]["model"] == "gemma3:4b"
         assert status["gemini"]["available"] is True
-        assert status["gemini"]["model"] == "gemini-2.5-flash"
+        assert status["gemini"]["model"] == TEST_MODEL_NAME
         assert "cache" in status
 
     @patch("madspark.llm.router._get_ollama_provider")
diff --git a/tests/test_llm_router_batch.py b/tests/test_llm_router_batch.py
index 7fd04bdf..bfb619ef 100644
--- a/tests/test_llm_router_batch.py
+++ b/tests/test_llm_router_batch.py
@@ -9,6 +9,7 @@
 from unittest.mock import Mock, patch
 from pydantic import BaseModel, Field
 from typing import List
+from .test_constants import TEST_MODEL_NAME
 
 from madspark.llm.router import LLMRouter, reset_router
 from madspark.llm.response import LLMResponse
@@ -90,7 +91,7 @@ def mock_gemini_provider():
     """Create mock Gemini provider."""
     provider = Mock()
     provider.provider_name = "gemini"
-    provider.model_name = "gemini-2.5-flash"
+    provider.model_name = "TEST_MODEL_NAME"
     provider.health_check.return_value = True
     provider.supports_multimodal = True
 
@@ -103,7 +104,7 @@ def generate_side_effect(prompt, schema, **kwargs):
                 LLMResponse(
                     text=f'{{"score": {6.0 + call_count[0]}, "comment": "Gemini eval {call_count[0]}"}}',
                     provider="gemini",
-                    model="gemini-2.5-flash",
+                    model="TEST_MODEL_NAME",
                     tokens_used=100 + call_count[0] * 10,
                     latency_ms=200 * call_count[0],
                     cost=0.00001 * call_count[0],
diff --git a/tests/test_logical_inference_structured_output.py b/tests/test_logical_inference_structured_output.py
index c8d5dbf3..163d464c 100644
--- a/tests/test_logical_inference_structured_output.py
+++ b/tests/test_logical_inference_structured_output.py
@@ -6,6 +6,7 @@
 
 from unittest.mock import Mock, patch
 import json
+from .test_constants import TEST_MODEL_NAME
 
 
 class TestFullAnalysisStructuredOutput:
@@ -17,7 +18,7 @@ def test_full_analysis_uses_structured_output_config(self, mock_get_model):
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
         from google.genai import types
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         # Create mock GenAI client
         mock_client = Mock()
@@ -55,7 +56,7 @@ def test_full_analysis_parses_json_response(self, mock_get_model):
         """Should parse JSON response for full analysis."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -91,7 +92,7 @@ def test_causal_analysis_uses_structured_output(self, mock_get_model):
         """Should use structured output for causal analysis."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -135,7 +136,7 @@ def test_constraint_analysis_uses_structured_output(self, mock_get_model):
         """Should use structured output for constraint analysis."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -183,7 +184,7 @@ def test_contradiction_analysis_with_contradictions(self, mock_get_model):
         """Should parse contradictions from structured output."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -225,7 +226,7 @@ def test_contradiction_analysis_no_contradictions(self, mock_get_model):
         """Should handle case with no contradictions."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -263,7 +264,7 @@ def test_implications_analysis_uses_structured_output(self, mock_get_model):
         """Should use structured output for implications."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -369,7 +370,7 @@ def test_batch_analysis_uses_structured_output(self, mock_get_model):
         """Should use structured output for batch analysis."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -417,7 +418,7 @@ def test_invalid_json_raises_clear_error(self, mock_get_model):
         """Should handle invalid JSON gracefully with fallback to text parsing."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -445,7 +446,7 @@ def test_missing_required_fields_handled(self, mock_get_model):
         """Should handle missing required fields."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
@@ -481,7 +482,7 @@ def test_existing_test_patterns_still_work(self, mock_get_model):
         """Existing test patterns should continue working."""
         from madspark.utils.logical_inference_engine import LogicalInferenceEngine, InferenceType
 
-        mock_get_model.return_value = "gemini-1.5-flash"
+        mock_get_model.return_value = "TEST_MODEL_NAME"
 
         mock_client = Mock()
         mock_response = Mock()
diff --git a/tests/test_skeptic_batch.py b/tests/test_skeptic_batch.py
index 690985f8..9e91fb75 100644
--- a/tests/test_skeptic_batch.py
+++ b/tests/test_skeptic_batch.py
@@ -1,6 +1,7 @@
 """Tests for batch skeptic functionality."""
 import pytest
 from unittest.mock import Mock, patch
+from .test_constants import TEST_MODEL_NAME
 
 try:
     from madspark.agents.skeptic import criticize_ideas_batch
@@ -18,7 +19,7 @@ class TestSkepticBatch:
     @patch('madspark.agents.skeptic.get_model_name')
     def test_criticize_ideas_batch_single(self, mock_model_name, mock_client):
         """Test batch skeptic with single idea."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         # Mock response
         mock_response = Mock()
@@ -56,7 +57,7 @@ def test_criticize_ideas_batch_single(self, mock_model_name, mock_client):
     @patch('madspark.agents.skeptic.get_model_name')
     def test_criticize_ideas_batch_multiple(self, mock_model_name, mock_client):
         """Test batch skeptic with multiple ideas."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         # Mock response for 3 ideas
         mock_response = Mock()
@@ -123,7 +124,7 @@ def test_criticize_ideas_batch_empty_list(self, mock_client):
     @patch('madspark.agents.skeptic.get_model_name')
     def test_criticize_ideas_batch_invalid_json(self, mock_model_name, mock_client):
         """Test handling of invalid JSON response."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         mock_response = Mock()
         mock_response.text = "Invalid JSON response"
@@ -139,7 +140,7 @@ def test_criticize_ideas_batch_invalid_json(self, mock_model_name, mock_client):
     @patch('madspark.agents.skeptic.get_model_name')
     def test_criticize_ideas_batch_api_error(self, mock_model_name, mock_client):
         """Test handling of API errors."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         mock_client.models.generate_content.side_effect = Exception("API Error")
         
         ideas = [{"idea": "Test", "advocacy": "Test advocacy"}]
@@ -152,7 +153,7 @@ def test_criticize_ideas_batch_api_error(self, mock_model_name, mock_client):
     @patch('madspark.agents.skeptic.get_model_name')
     def test_criticize_ideas_batch_formatted_output(self, mock_model_name, mock_client):
         """Test that batch results include formatted text output."""
-        mock_model_name.return_value = "gemini-2.5-flash"
+        mock_model_name.return_value = "TEST_MODEL_NAME"
         
         mock_response = Mock()
         mock_response.text = '''[{
diff --git a/tests/test_structured_output.py b/tests/test_structured_output.py
index bde9e2d9..ad7135ec 100644
--- a/tests/test_structured_output.py
+++ b/tests/test_structured_output.py
@@ -5,6 +5,7 @@
 """
 import json
 from unittest.mock import Mock, patch
+from .test_constants import TEST_MODEL_NAME
 
 from madspark.agents.idea_generator import (
     improve_idea,
@@ -41,7 +42,7 @@ def test_improvement_prompt_format_requirements(self):
         assert "no meta-commentary" in prompt.lower() or "start directly" in prompt.lower()
     
     @patch('madspark.agents.idea_generator.idea_generator_client')
-    @patch('madspark.agents.idea_generator.model_name', 'gemini-2.5-flash')
+    @patch('madspark.agents.idea_generator.model_name', 'TEST_MODEL_NAME')
     @patch('madspark.agents.idea_generator.GENAI_AVAILABLE', True)
     def test_structured_output_configuration(self, mock_client):
         """Test that structured output is configured when available."""
diff --git a/tests/test_structured_output_integration.py b/tests/test_structured_output_integration.py
index 82148f13..9137624f 100644
--- a/tests/test_structured_output_integration.py
+++ b/tests/test_structured_output_integration.py
@@ -7,6 +7,7 @@
 import json
 import pytest
 from unittest.mock import Mock, patch
+from .test_constants import TEST_MODEL_NAME
 
 # Test that the system loads and uses structured output
 class TestStructuredOutputIntegration:
@@ -14,7 +15,7 @@ class TestStructuredOutputIntegration:
     
     @patch('madspark.agents.idea_generator.GENAI_AVAILABLE', True)
     @patch('madspark.agents.idea_generator.idea_generator_client')
-    @patch('madspark.agents.idea_generator.model_name', 'gemini-2.5-flash')
+    @patch('madspark.agents.idea_generator.model_name', 'TEST_MODEL_NAME')
     def test_improve_idea_uses_structured_output(self, mock_client):
         """Test that improve_idea function uses structured output when available."""
         from madspark.agents.idea_generator import improve_idea
diff --git a/web/.env.example b/web/.env.example
index 48623370..dc468bf4 100644
--- a/web/.env.example
+++ b/web/.env.example
@@ -1,7 +1,7 @@
 # Google Cloud API configuration
 # SECURITY: Replace with real values for your environment
 GOOGLE_API_KEY=your-api-key-here-replace-with-real-key
-GOOGLE_GENAI_MODEL=gemini-2.5-flash
+GOOGLE_GENAI_MODEL=gemini-3-flash-preview
 GOOGLE_CLOUD_PROJECT=your-project-id-here
 
 # Redis Configuration (optional, for distributed caching)
diff --git a/web/README.md b/web/README.md
index ca211a2c..c053d8dc 100644
--- a/web/README.md
+++ b/web/README.md
@@ -119,7 +119,7 @@ When `MADSPARK_LLM_PROVIDER=auto` (default), the router uses:
 | Provider | Models | Cost | Use Case |
 |----------|--------|------|----------|
 | **Ollama** | gemma3:4b (fast, 3.3GB)<br>gemma3:12b (balanced, 8.1GB) | FREE | Text-only, images (local) |
-| **Gemini** | gemini-2.5-flash | Paid | PDFs, URLs, fallback |
+| **Gemini** | gemini-3-flash-preview | Paid | PDFs, URLs, fallback |
 
 ### Model Tiers
 
@@ -127,7 +127,7 @@ Configure via UI "Advanced LLM Settings" or `MADSPARK_MODEL_TIER`:
 
 - **Fast**: gemma3:4b (3.3GB) - Quick responses (~10s)
 - **Balanced** (default): gemma3:12b (8.1GB) - Better quality (~20s)
-- **Quality**: gemini-2.5-flash - Best results (cloud, paid)
+- **Quality**: gemini-3-flash-preview - Best results (cloud, paid)
 
 ### LLM Usage Statistics
 
@@ -492,7 +492,7 @@ The web interface expects these fields in API responses:
 |----------|-------------|---------|
 | `MADSPARK_MODE` | `api` (production) or `mock` (testing) | `api` |
 | `GOOGLE_API_KEY` | Your Gemini API key (optional with Ollama) | `test_api_key` |
-| `GOOGLE_GENAI_MODEL` | Gemini model to use | `gemini-2.5-flash` |
+| `GOOGLE_GENAI_MODEL` | Gemini model to use | `gemini-3-flash-preview` |
 | `REDIS_URL` | Redis connection | `redis://redis:6379/0` |
 
 ### LLM Router Settings
diff --git a/web/docker-compose.yml b/web/docker-compose.yml
index b236e57d..80ca918f 100644
--- a/web/docker-compose.yml
+++ b/web/docker-compose.yml
@@ -19,7 +19,7 @@ services:
       # Use environment variable substitution with defaults for CI/testing
       # Values will be read from host environment or use defaults if not set
       GOOGLE_API_KEY: ${GOOGLE_API_KEY:-test_api_key}
-      GOOGLE_GENAI_MODEL: ${GOOGLE_GENAI_MODEL:-gemini-2.5-flash}
+      GOOGLE_GENAI_MODEL: ${GOOGLE_GENAI_MODEL:-gemini-3-flash-preview}
       GOOGLE_CLOUD_PROJECT: ${GOOGLE_CLOUD_PROJECT:-test_project}
       # Default to 'api' mode for Ollama-first architecture (free local inference)
       # Set MADSPARK_MODE=mock explicitly if you want mock mode
diff --git a/web/frontend/src/components/IdeaGenerationForm.tsx b/web/frontend/src/components/IdeaGenerationForm.tsx
index 64644141..3e81d727 100644
--- a/web/frontend/src/components/IdeaGenerationForm.tsx
+++ b/web/frontend/src/components/IdeaGenerationForm.tsx
@@ -477,7 +477,7 @@ const IdeaGenerationForm: React.FC<IdeaGenerationFormProps> = ({
                 >
                   <option value="fast">Fast (gemma3:4b - Quick responses)</option>
                   <option value="balanced">Balanced (gemma3:12b - Better quality)</option>
-                  <option value="quality">Quality (gemini-2.5-flash - Best results)</option>
+                  <option value="quality">Quality (gemini-3-flash-preview - Best results)</option>
                 </select>
                 <p className="mt-1 text-xs text-gray-500">
                   Higher tiers provide better results but may be slower or cost more
