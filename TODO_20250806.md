# TODO: MadSpark Workflow Improvements - August 6, 2025

## Executive Summary

This document outlines critical improvements needed for the MadSpark multi-agent workflow based on analysis of the current implementation. The main issues are:
1. Confusing parameter naming across interfaces and agents
2. Sequential operations that could run in parallel
3. Missing information flow between agents
4. Prompts using manual formatting instead of structured output
5. Evaluation prompts producing overly brief feedback

## Current Workflow Analysis

### Parameter Flow (Current - Confusing)
```
Web UI          Backend         Idea Gen        Critic
------          -------         --------        ------
Topic     ‚Üí     theme      ‚Üí    topic      ‚Üí    context
Context   ‚Üí     constraints ‚Üí   context    ‚Üí    criteria
```

### Current Agent Dependencies
1. **Idea Generation** ‚Üê User input (topic, context)
2. **Initial Evaluation** ‚Üê Generated ideas + constraints as criteria
3. **Multi-dimensional Evaluation** ‚Üê Ideas + theme + constraints (sequential with initial eval)
4. **Logical Inference** ‚Üê Evaluated candidates + theme + constraints
5. **Advocacy** ‚Üê Ideas + evaluation critique only (NO constraints)
6. **Skepticism** ‚Üê Ideas + evaluation critique only (NO constraints)
7. **Improvement** ‚Üê Ideas + critique + advocacy + skepticism (NO constraints, NO logical inference)
8. **Re-evaluation** ‚Üê Improved ideas + constraints as criteria + biased context

### Current Prompts

#### Idea Generator
```python
prompt_template = f"""{LANGUAGE_CONSISTENCY_INSTRUCTION}Use the user's main prompt and context below to {IDEA_GENERATION_INSTRUCTION}.
Make sure the ideas are actionable and innovative.

IMPORTANT FORMAT REQUIREMENTS:
- Generate exactly 5 diverse ideas
- Start your response directly with "1." (no introductory text)
- Keep each idea concise (2-3 sentences maximum)
- Number each idea clearly (1., 2., 3., 4., 5.)
...
```
**Problem**: Manual formatting instructions despite using structured output

#### Critic (Evaluator)
```python
CRITIC_SYSTEM_INSTRUCTION = (
    "You are an expert critic. Evaluate the given ideas based on the"
    " provided criteria and context. Provide constructive feedback and"
    " identify potential weaknesses. Always respond in the same language"
    " as the input provided."
)
# Prompt includes: "a concise string explaining your reasoning"
```
**Problem**: "concise" instruction results in unhelpfully brief feedback

#### Re-evaluation
```python
improved_context = (
    f"{theme}\n"
    f"[These are IMPROVED versions of ideas that have been refined based on:\n"
    f"- Critical analysis identifying weaknesses\n"
    f"- Advocacy highlighting strengths\n"
    f"- Skeptical evaluation of risks\n"
    f"- Specific improvements to address all feedback]\n"
    f"Please evaluate each improved idea considering the enhancements made."
)
```
**Problem**: Biases the evaluator by revealing it's an "improved" version

## Recommended Changes (Prioritized)

### 1. **Standardize Parameter Names** (HIGH PRIORITY)
Use "topic" and "context" consistently across all layers:
- Web UI: Topic, Context (already correct)
- Backend: `topic`, `context` (not `theme`, `constraints`)
- All agents: `topic`, `context`
- Evaluation functions: Pass context as `context`, not `criteria`

### 2. **Fix Display Names** (HIGH PRIORITY)
- Rename "üß† Enhanced Analysis" to "üß† Multi-dimensional Analysis"
- This avoids confusion with "enhanced reasoning" (advocacy/skepticism)

### 3. **Implement Parallel Processing** (HIGH PRIORITY)
Phase 1 parallel operations:
```python
# Run these concurrently (no interdependencies):
logical_task = asyncio.create_task(batch_logical_inference(candidates))
advocacy_task = asyncio.create_task(batch_advocacy(candidates))
skepticism_task = asyncio.create_task(batch_skepticism(candidates))
multi_dim_task = asyncio.create_task(batch_multi_dimensional_eval(ideas))

# Wait for all to complete
logical_results, advocacy_results, skepticism_results, multi_dim_results = await asyncio.gather(
    logical_task, advocacy_task, skepticism_task, multi_dim_task
)
```

### 4. **Fix Information Flow to Agents** (HIGH PRIORITY)
Each agent should receive:
- **Advocacy**: `{idea, evaluation, context, multi_dimensional_analysis}`
- **Skepticism**: `{idea, evaluation, context, multi_dimensional_analysis}`
- **Improvement**: `{idea, critique, advocacy, skepticism, context, logical_inference}`
- **Logical Inference**: Already correct (gets context)

### 5. **Remove Manual Formatting from All Prompts** (MEDIUM PRIORITY)
Update all prompts to rely on structured output schemas:
- Remove "Generate exactly 5 diverse ideas" ‚Üí Use `num_ideas` parameter
- Remove "Start your response directly with '1.'" ‚Üí Use JSON schema
- Remove "Number each idea clearly" ‚Üí Use structured array response
- Update IDEA_GENERATOR_SCHEMA to accept variable count

### 6. **Improve Evaluation Feedback Quality** (MEDIUM PRIORITY)
Update CRITIC_SYSTEM_INSTRUCTION:
```python
CRITIC_SYSTEM_INSTRUCTION = (
    "You are an expert critic. Evaluate the given ideas based on the"
    " provided criteria and context. Provide detailed, constructive feedback"
    " that thoroughly analyzes strengths, weaknesses, and potential improvements."
    " Your evaluation should be comprehensive and actionable."
    " Always respond in the same language as the input provided."
)
```
Remove "concise" from all evaluation prompts.

### 7. **Fix Re-evaluation Bias** (MEDIUM PRIORITY)
Don't reveal that ideas are "improved versions":
```python
# Simple, unbiased context:
improved_context = f"Topic: {topic}\nContext: {context}"

# Let the re-evaluation judge ideas on their own merit
```

### 8. **Batch Multi-dimensional Evaluation** (LOW PRIORITY)
Currently evaluates one idea at a time. Should batch all ideas in one API call for efficiency.

### 9. **Dynamic Idea Generation Count** (LOW PRIORITY)
Instead of hardcoded 5 ideas:
- Generate `max(5, num_top_candidates + 2)` ideas
- This ensures enough ideas after filtering

## Implementation Notes

1. **Breaking Changes**: Parameter renaming will require updates to:
   - Web frontend API calls
   - Backend API endpoints
   - All agent functions
   - Test files

2. **Backward Compatibility**: Consider supporting old parameter names temporarily with deprecation warnings

3. **Testing Requirements**:
   - Update all integration tests for new parameter names
   - Add tests for parallel execution
   - Verify information flow between agents
   - Test unbiased re-evaluation

4. **Documentation Updates**:
   - Update API documentation
   - Update web/README.md
   - Update inline code comments

## Priority Order for Implementation

1. Fix parameter naming consistency (foundational change)
2. Fix information flow to all agents
3. Implement parallel processing for performance
4. Update prompts for better feedback quality
5. Remove formatting instructions from prompts
6. Fix re-evaluation bias
7. Implement remaining optimizations

## Success Metrics

- Clearer, more maintainable code with consistent naming
- 30-40% reduction in workflow execution time from parallelization
- More detailed and actionable evaluation feedback
- Unbiased re-evaluation scores
- All agents have access to necessary context for better decisions