2025-11-18 22:09:10 - INFO - Using default constraints
2025-11-18 22:09:10 - INFO - Temperature Settings:
  Base: 0.7
  Idea Generation: 0.9
  Evaluation: 0.3
  Advocacy: 0.5
  Skepticism: 0.5
2025-11-18 22:09:10 - INFO - Running MadSpark workflow with theme: 'renewable energy solutions'
2025-11-18 22:09:10 - INFO - Constraints: Generate practical and innovative ideas
2025-11-18 22:09:10 - INFO - Using synchronous execution
2025-11-18 22:09:10 - INFO - Enhanced reasoning engine initialized
2025-11-18 22:09:10 - INFO - Router initialized: primary=auto, tier=fast, fallback=True, cache=True
2025-11-18 22:09:10 - INFO - Initialized cache at /Users/yuyamukai/.cache/madspark/llm (max size: 1000MB)
2025-11-18 22:09:11 - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-11-18 22:09:11 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models "HTTP/1.1 200 OK"
2025-11-18 22:09:11 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:09:18 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 22:09:18 - INFO - AFC remote call 1 is done.
2025-11-18 22:09:18 - INFO - Gemini generated structured output in 7633ms (1176 tokens, $0.000235)
2025-11-18 22:09:18 - ERROR - Cache set failed: Cannot cache object of type list. Expected BaseModel or dict.
2025-11-18 22:09:18 - INFO - Generated via gemini in 7634ms (1176 tokens)
2025-11-18 22:09:18 - INFO - Router generated ideas via gemini (1176 tokens)
2025-11-18 22:09:18 - INFO - Generated 5 ideas
2025-11-18 22:09:18 - INFO - Batch idea_generation: 10 items in 7.82s, 790 tokens, ~$0.0022
2025-11-18 22:09:18 - INFO - Generated 5 ideas
2025-11-18 22:09:18 - INFO - Novelty filter processed 5 ideas: 5 novel, 0 duplicates/similar
2025-11-18 22:09:18 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:09:32 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 22:09:32 - INFO - AFC remote call 1 is done.
2025-11-18 22:09:32 - INFO - Gemini generated structured output in 13403ms (3270 tokens, $0.000654)
2025-11-18 22:09:32 - ERROR - Cache set failed: Cannot cache object of type list. Expected BaseModel or dict.
2025-11-18 22:09:32 - INFO - Generated via gemini in 13404ms (3270 tokens)
2025-11-18 22:09:32 - INFO - Router generated evaluation via gemini (3270 tokens)
2025-11-18 22:09:32 - INFO - Batch evaluation: 5 items in 13.41s, 1894 tokens, ~$0.0052
2025-11-18 22:09:32 - INFO - Selected 1 top candidates for further processing.
2025-11-18 22:09:32 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:09:38 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 22:09:38 - INFO - AFC remote call 1 is done.
2025-11-18 22:09:38 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:09:40 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 22:09:40 - INFO - AFC remote call 1 is done.
2025-11-18 22:09:40 - INFO - Added multi-dimensional evaluation to 1 candidates
2025-11-18 22:09:40 - INFO - Batch multi_dimensional: 1 items in 8.60s
2025-11-18 22:09:40 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:09:41 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 503 Service Unavailable"
2025-11-18 22:09:41 - ERROR - Batch advocate API call failed: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
Traceback (most recent call last):
  File "/Users/yuyamukai/Eureka/src/madspark/agents/advocate.py", line 330, in advocate_ideas_batch
    response = advocate_client.models.generate_content(
        model=model_name,
        contents=prompt,
        config=config
    )
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/google/genai/models.py", line 5778, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/google/genai/models.py", line 4716, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/google/genai/_api_client.py", line 1077, in request
    response = self._request(http_request, stream=False)
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/google/genai/_api_client.py", line 968, in _request
    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/google/genai/_api_client.py", line 958, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/Users/yuyamukai/Eureka/venv/lib/python3.13/site-packages/google/genai/errors.py", line 107, in raise_for_response
    raise ServerError(status_code, response_json, response)
google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
2025-11-18 22:09:41 - ERROR - Batch advocate failed: Batch advocate failed: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
2025-11-18 22:09:41 - INFO - Batch advocate: 1 items in 0.64s
2025-11-18 22:09:41 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:09:55 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 22:09:55 - INFO - AFC remote call 1 is done.
2025-11-18 22:09:55 - INFO - Batch skeptic: 1 items in 13.81s, 2457 tokens, ~$0.0068
2025-11-18 22:09:55 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:10:13 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 22:10:13 - INFO - AFC remote call 1 is done.
2025-11-18 22:10:13 - INFO - Batch improve: 1 items in 18.59s, 4311 tokens, ~$0.0119
2025-11-18 22:10:13 - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-11-18 22:10:13 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models "HTTP/1.1 200 OK"
2025-11-18 22:10:13 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:10:14 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 503 Service Unavailable"
2025-11-18 22:10:14 - WARNING - Router error, falling back to direct API: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
2025-11-18 22:10:14 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:10:15 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 503 Service Unavailable"
2025-11-18 22:10:15 - WARNING - Attempt 1/4 failed for critic: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}. Retrying in 2.0 seconds...
2025-11-18 22:10:17 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:10:28 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 22:10:28 - INFO - AFC remote call 1 is done.
2025-11-18 22:10:28 - INFO - Gemini generated structured output in 11363ms (2888 tokens, $0.000578)
2025-11-18 22:10:28 - ERROR - Cache set failed: Cannot cache object of type list. Expected BaseModel or dict.
2025-11-18 22:10:28 - INFO - Generated via gemini in 11364ms (2888 tokens)
2025-11-18 22:10:28 - INFO - Router generated evaluation via gemini (2888 tokens)
2025-11-18 22:10:28 - INFO - Batch reevaluation: 1 items in 14.98s, 830 tokens, ~$0.0023
2025-11-18 22:10:28 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:10:37 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 22:10:37 - INFO - AFC remote call 1 is done.
2025-11-18 22:10:37 - INFO - AFC is enabled with max remote calls: 10.
2025-11-18 22:10:42 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 22:10:42 - INFO - AFC remote call 1 is done.
2025-11-18 22:10:42 - INFO - Added multi-dimensional evaluation to 1 candidates
2025-11-18 22:10:42 - INFO - Batch multi_dimensional: 1 items in 13.28s
2025-11-18 22:10:42 - INFO - Batch Performance: 8/8 calls successful, 21 items in 91.13s
2025-11-18 22:10:42 - INFO - Estimated cost: $0.0283
2025-11-18 22:10:42 - INFO - Bookmarking requested. Processing 1 results...
2025-11-18 22:10:42 - INFO - Loaded 218 bookmarks from examples/data/bookmarks.json
2025-11-18 22:10:42 - INFO - Bookmarked idea: Intelligent Distributed Energy Resource (IDER) Hub... (ID: bookmark_20251118_221042_307aca5a)
2025-11-18 22:10:42 - INFO - Bookmarked result as bookmark_20251118_221042_307aca5a
2025-11-18 22:10:42 - INFO - Auto-saved long output to output/markdown/madspark_renewable_energy_solutions_20251118_221042.md
‚úÖ API key found. Running with Google Gemini API...


üöÄ Generating ideas with AI model...
‚è≥ This may take 30-60 seconds for quality results...

‚úÖ Bookmarked result (ID: bookmark_20251118_221042_307aca5a)
================================================================================
MADSPARK MULTI-AGENT IDEA GENERATION RESULTS
================================================================================


... [Output truncated - 153 more lines]
(Full output saved to file - use --output-file to specify location)

üìÑ Full output saved to: output/markdown/madspark_renewable_energy_solutions_20251118_221042.md
