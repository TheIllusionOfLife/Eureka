Total generate_content calls: 19

✅ WITH structured output (config + mime_type/schema): 12
   src/madspark/core/enhanced_reasoning.py:805
   src/madspark/core/enhanced_reasoning.py:993
   src/madspark/agents/advocate.py:219
   src/madspark/agents/advocate.py:330
   src/madspark/agents/idea_generator.py:305
   src/madspark/agents/idea_generator.py:678
   src/madspark/agents/skeptic.py:211
   src/madspark/agents/skeptic.py:323
   src/madspark/agents/structured_idea_generator.py:211
   src/madspark/agents/critic.py:219
   src/madspark/utils/logical_inference_engine.py:136
   src/madspark/utils/logical_inference_engine.py:215

❌ WITHOUT structured output: 7

   src/madspark/core/enhanced_reasoning.py:893
   Has config param: False
   Context:
                  try:
                      # Call without config - testing showed this works better
                      response = self.genai_client.models.generate_content(
                          model=get_model_name(),
                          contents=prompt
                      )

   src/madspark/agents/idea_generator.py:532
   Has config param: True
   Context:
          )
      
          response = idea_generator_client.models.generate_content(
              model=model_name,
              contents=contents,
              config=config

   src/madspark/agents/structured_idea_generator.py:279
   Has config param: True
   Context:
              )
              
              response = genai_client.models.generate_content(
                  model=model_name,
                  contents=prompt,
                  config=config

   src/madspark/utils/content_safety.py:154
   Has config param: True
   Context:
                      
                      # Attempt generation
                      response = client.models.generate_content(
                          model=model_name,
                          contents=safe_prompt,
                          config=generation_config

   src/madspark/llm/providers/gemini.py:259
   Has config param: True
   Context:
      
              start = time.time()
              response = self.client.models.generate_content(
                  model=self._model, contents=contents, config=config
              )
              latency = (time.time() - start) * 1000

   src/madspark/llm/providers/gemini.py:377
   Has config param: True
   Context:
      
              start = time.time()
              response = self.client.models.generate_content(
                  model=self._model, contents=contents, config=config
              )
              latency = (time.time() - start) * 1000

   src/madspark/llm/providers/gemini.py:448
   Has config param: True
   Context:
      
              try:
                  response = self.client.models.generate_content(
                      model=self._model, contents=contents, config=config
                  )
      
