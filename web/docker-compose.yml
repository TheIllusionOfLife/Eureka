# Docker Compose configuration for MadSpark web interface
# Compatible with Docker Compose v2+ (no version field needed)

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      # Use environment variable substitution with defaults for CI/testing
      # Values will be read from host environment or use defaults if not set
      GOOGLE_API_KEY: ${GOOGLE_API_KEY:-test_api_key}
      GOOGLE_GENAI_MODEL: ${GOOGLE_GENAI_MODEL:-gemini-2.5-flash}
      GOOGLE_CLOUD_PROJECT: ${GOOGLE_CLOUD_PROJECT:-test_project}
      # Default to 'api' mode for Ollama-first architecture (free local inference)
      # Set MADSPARK_MODE=mock explicitly if you want mock mode
      MADSPARK_MODE: ${MADSPARK_MODE:-api}
      MADSPARK_PATH: /madspark
      REDIS_URL: redis://redis:6379/0
      # LLM Router configuration (Ollama-first by default)
      OLLAMA_HOST: http://ollama:11434
      MADSPARK_LLM_PROVIDER: ${MADSPARK_LLM_PROVIDER:-auto}
      MADSPARK_MODEL_TIER: ${MADSPARK_MODEL_TIER:-fast}
      MADSPARK_CACHE_ENABLED: ${MADSPARK_CACHE_ENABLED:-true}
      MADSPARK_CACHE_DIR: /cache/llm
    volumes:
      - ./backend:/app
      - ../:/madspark  # Mount parent directory for MadSpark imports
      - llm-cache:/cache/llm  # LLM response cache
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_healthy
    networks:
      - madspark-network

  ollama:
    # Image version pinned for stability. Python SDK (requirements.txt) uses minimum version >=0.4.1
    image: ollama/ollama:0.12.9
    ports:
      - "11434:11434"
    volumes:
      # Mount local Ollama directory to reuse existing models (avoids re-downloading ~13GB)
      # Falls back to Docker volume if ~/.ollama doesn't exist
      - ${OLLAMA_MODELS_DIR:-~/.ollama}:/root/.ollama
      - ./scripts/pull-models.sh:/usr/local/bin/pull-models.sh:ro
    networks:
      - madspark-network
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    # Auto-pull Ollama models on first startup using dedicated script (prevents shell injection)
    # Models: gemma3:4b-it-qat (fast, 4GB) and gemma3:12b-it-qat (balanced, 8.9GB)
    entrypoint: ["/usr/local/bin/pull-models.sh"]
    # GPU support: Uncomment the deploy section below for NVIDIA GPU acceleration
    # For CPU-only systems, leave this commented out
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - REACT_APP_API_URL=http://localhost:8000
    depends_on:
      - backend
    networks:
      - madspark-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - madspark-network

volumes:
  redis-data:
  llm-cache:      # LLM response cache for faster repeated queries

networks:
  madspark-network:
    driver: bridge