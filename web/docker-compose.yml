# Docker Compose configuration for MadSpark web interface
# Compatible with Docker Compose v2+ (no version field needed)

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      # Use environment variable substitution with defaults for CI/testing
      # Values will be read from host environment or use defaults if not set
      GOOGLE_API_KEY: ${GOOGLE_API_KEY:-test_api_key}
      GOOGLE_GENAI_MODEL: ${GOOGLE_GENAI_MODEL:-gemini-2.5-flash}
      GOOGLE_CLOUD_PROJECT: ${GOOGLE_CLOUD_PROJECT:-test_project}
      MADSPARK_MODE: ${MADSPARK_MODE:-mock}
      MADSPARK_PATH: /madspark
      REDIS_URL: redis://redis:6379/0
      # LLM Router configuration (Ollama-first by default)
      OLLAMA_HOST: http://ollama:11434
      MADSPARK_LLM_PROVIDER: ${MADSPARK_LLM_PROVIDER:-auto}
      MADSPARK_MODEL_TIER: ${MADSPARK_MODEL_TIER:-fast}
      MADSPARK_CACHE_ENABLED: ${MADSPARK_CACHE_ENABLED:-true}
      MADSPARK_CACHE_DIR: /cache/llm
    volumes:
      - ./backend:/app
      - ../:/madspark  # Mount parent directory for MadSpark imports
      - llm-cache:/cache/llm  # LLM response cache
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_healthy
    networks:
      - madspark-network

  ollama:
    image: ollama/ollama:0.5.0
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    networks:
      - madspark-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:11434/"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    # Pull models on startup (optional, can be done manually)
    # entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 5 && ollama pull gemma3:4b-it-qat && wait"]
    # GPU support: Uncomment the deploy section below for NVIDIA GPU acceleration
    # For CPU-only systems, leave this commented out
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - REACT_APP_API_URL=http://localhost:8000
    depends_on:
      - backend
    networks:
      - madspark-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - madspark-network

volumes:
  redis-data:
  ollama-models:  # Persistent storage for Ollama models
  llm-cache:      # LLM response cache for faster repeated queries

networks:
  madspark-network:
    driver: bridge